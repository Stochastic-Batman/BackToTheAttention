{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **BackToTheAttention**\n",
    "\n",
    "This project aims to use the attention mechanism for time series prediction. As time series data needs special handling and I had no experience in doing so, I decided to use one of the most beautiful, influential, and genius mechanisms ever to combine it with stock prediction. I will try to predict stocks for Pinterest (ticker: \"PINS\"), but one can reuse this code for any stock.\n",
    "\n",
    "This notebook, as well as its successor: `inference.ipynb`, follows a **code-first** structure, meaning I first implemented the code under the `src/` directory and then simply copy-pasted the code here, so some aspects might have been implemented differently if the project were notebook-oriented.\n",
    "\n",
    "**Note**: Since this script uses Yahoo Finance's API to get the stock data, it needs an internet connection to run. Although installing the libraries also requires an internet connection, that might not be too project-specific.\n",
    "\n",
    "First, let's start with library installation, imports, and logger setup:"
   ],
   "id": "9bf90f91cc7ff756"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.253558Z",
     "start_time": "2025-12-21T14:43:58.477124Z"
    }
   },
   "source": [
    "!pip install scikit-learn torch yfinance\n",
    "import argparse\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%d/%m/%Y %H:%M:%S')\n",
    "logger = logging.getLogger(\"BackToTheTrainLogger (Train)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: torch in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: yfinance in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (0.2.66)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from scikit-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (2.32.5)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (4.5.1)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (2.4.7)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (3.18.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (4.14.3)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (0.14.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (6.33.2)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8.1)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.11.12)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from requests>=2.31->yfinance) (2.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ladoturmanidze\\backtotheattention\\btta_venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Data Acquisition**\n",
    "\n",
    "We have to define a method for acquiring the data for any stock and custom period:"
   ],
   "id": "619572743ff1cc31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.291358Z",
     "start_time": "2025-12-21T14:44:04.268173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data(ticker: str, period: str = \"3y\") -> pd.DataFrame:\n",
    "    data = yf.Ticker(ticker)\n",
    "    df = data.history(period=period)  # only the latest 'period' stocks\n",
    "    logger.info(\"HISTORY:\\n%s\\n%s\", list(df.columns), df.head())  # we want to predict the 'Close' price\n",
    "\n",
    "    # we don't really care about dividends and stock splits, as they provide no information related to the 'Close' price (most of the time they are 0).\n",
    "    # I am not a quant, but this is based on my limited knowledge; therefore, we drop those two columns and use the rest for the task.\n",
    "    df.drop(columns=[\"Dividends\", \"Stock Splits\"], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "809235701851d072",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "MinMax scaling is necessary for this task because neural network based architectures, including Transformers, train more stably and converge faster when input features are normalized to a consistent range like $[0,1]$ rather than using raw stock prices that can vary widely in magnitude. Large or varying input values can lead to exploding or vanishing gradients during backpropagation, particularly in attention mechanisms where dot products between queries and keys become unbalanced and dominate the softmax unfairly. Scaling also ensures that the model's learned patterns are driven by relative changes in price rather than absolute levels, which is especially helpful for financial time series.\n",
    "\n",
    "I fit `MinMaxScaler` (0 to 1) ONLY on training portion to prevent data leakage.\n",
    "$$x_\\text{scaled} = \\frac{x - \\text{train}_{\\text{min}}}{\\text{train}_{\\text{max}} - \\text{train}_{\\text{min}}}$$"
   ],
   "id": "101d48d06ae8f5ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.311018Z",
     "start_time": "2025-12-21T14:44:04.305466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def minmax_scale(series: np.ndarray, train_ratio: float = 0.8) -> tuple[np.ndarray, np.ndarray, MinMaxScaler]:\n",
    "    n = len(series)\n",
    "    split_idx = int(n * train_ratio)  # chronological split point\n",
    "    train_series = series[:split_idx]\n",
    "    test_series = series[split_idx:]\n",
    "\n",
    "    # fit scaler only on train data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(train_series)\n",
    "    train_scaled = scaler.transform(train_series)\n",
    "    test_scaled = scaler.transform(test_series)\n",
    "\n",
    "    logger.info(\"Train scaled shape: %s, Test scaled shape: %s\", train_scaled.shape, test_scaled.shape)\n",
    "    logger.info(\"Scaler min: %.4f, scale: %.4f\", scaler.min_[0], scaler.scale_[0])\n",
    "\n",
    "    return train_scaled, test_scaled, scaler"
   ],
   "id": "3af598a92be9e018",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the `create_windows` (sliding window) method, which for $X_i = [x_{i}, x_{i + 1}, \\cdots, x_{i + L - 1}] \\in \\mathbb{R}^{L \\times 1}$ and $y_i = x_{i + L}$ returns $X \\in \\mathbb{R}^{N \\times L \\times 1}, y \\in \\mathbb{R}^{N \\times 1}$ where $N =$ `len(scaled_data)`, a.k.a. lookback.",
   "id": "4323b853604e0d41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.324350Z",
     "start_time": "2025-12-21T14:44:04.320110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_windows(scaled_data: np.ndarray, L: int = 30) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if len(scaled_data) <= L:\n",
    "        raise ValueError(\"Data length %d must be > lookback %d\", len(scaled_data), L)\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_data) - L):\n",
    "        X.append(scaled_data[i:i + L])\n",
    "        y.append(scaled_data[i + L])\n",
    "\n",
    "    X = np.array(X)  # shape: (N, L, 1)\n",
    "    y = np.array(y).squeeze(-1)  # shape: (N,) for easier loss computation\n",
    "\n",
    "    logger.info(\"Created windows -> X shape: %s, y shape: %s\", X.shape, y.shape)\n",
    "\n",
    "    return X, y"
   ],
   "id": "f51ef8adc2e6d195",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "before moving on to the architecture, I finally define:",
   "id": "b5fcc3a2b92e9936"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.337639Z",
     "start_time": "2025-12-21T14:44:04.333160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_datasets(df: pd.DataFrame, L: int = 30, train_ratio: float = 0.8) -> tuple[\n",
    "    np.ndarray, np.ndarray, np.ndarray, np.ndarray, MinMaxScaler]:\n",
    "    series = df['Close'].values.reshape(-1, 1)\n",
    "    train_scaled, test_scaled, scaler = minmax_scale(series, train_ratio=train_ratio)\n",
    "\n",
    "    # test windows use ONLY past test data (no train leakage)\n",
    "    X_train, y_train = create_windows(train_scaled, L=L)\n",
    "    X_test, y_test = create_windows(test_scaled, L=L)\n",
    "\n",
    "    logger.info(\"Final datasets ready -> Train samples: %d, Test samples: %d\", len(X_train), len(X_test))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler"
   ],
   "id": "15d0ea4b55e34b4f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Embedding**\n",
    "\n",
    "I read the paper about the Rotary Positional Embedding in August 2025, and as it is better than simple absolute or relative embeddings, why not use it? RoPE is a mixture of absolute positional embedding, which was introduced in the original \"Attention is All You Need\" paper (where the base $10000$ comes from), and relative encoding (which is not really efficient to compute, as opposed to absolute and rotary). Because of the latter, I thought it would be a great fit for the time-series data."
   ],
   "id": "a4b7cbcf87b2dad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.353242Z",
     "start_time": "2025-12-21T14:44:04.347144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim: int, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "\n",
    "        # precompute frequencies: \\theta_i = base^{-2i/dim}\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(start=0, end=dim, step=2, dtype=torch.float32) / dim))\n",
    "\n",
    "        # to save as part of a model's persistent state without treating it as a learnable parameter\n",
    "        self.register_buffer(name=\"inv_freq\", tensor=inv_freq, persistent=True)  # shape: (dim / 2,)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> torch.Tensor:\n",
    "        positions = torch.arange(0, seq_len, device=x.device, dtype=self.inv_freq.dtype)  # (seq_len,)\n",
    "        angles = positions[:, None] * self.inv_freq[None, :]  # (seq_len, dim//2)\n",
    "        sin, cos = torch.sin(angles), torch.cos(angles)  # (seq_len, dim / 2)\n",
    "        # expand dims for broadcasting: (seq_len, dim//2) -> (1, seq_len, dim//2)\n",
    "        sin = sin.unsqueeze(0)\n",
    "        cos = cos.unsqueeze(0)\n",
    "\n",
    "        # split the last dimension\n",
    "        x1 = x[..., : self.dim // 2]  # (B, seq_len, dim / 2)\n",
    "        x2 = x[..., self.dim // 2:]  # (B, seq_len, dim / 2)\n",
    "        # Rotation matrix\n",
    "        rotated_x1 = x1 * cos - x2 * sin\n",
    "        rotated_x2 = x1 * sin + x2 * cos\n",
    "\n",
    "        return torch.cat([rotated_x1, rotated_x2], dim=-1)"
   ],
   "id": "81e079fcb7480765",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **DeLorean + Attention = DeTention**\n",
    "\n",
    "`DeTentionBlock` is the core repeating unit that applies RoPE-enhanced self-attention and feed-forward processing with residual connections. I won't be repeating what arguments this class takes, as that's obvious, but here is my though process for the architecture:\n",
    "\n",
    "Apply embedding to the normalized layer, then compute the attention with RoPE-embedded $q$ and $k$ (not $v$!), normalize again and combine with the original input to allow the gradient flow, then use feed-forward neural net to figure out the \"final\" token.\n",
    "\n",
    "**Note**: as we use GLU, it requires `ff_hidden_size` to be of even dimension. If you do not want to have such restriction, maybe try replacing GLU with GELU."
   ],
   "id": "b7c9d979e4b0d814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.368649Z",
     "start_time": "2025-12-21T14:44:04.361946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeTentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, ff_hidden_size: int = 256, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # must be even and divisible by n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_hidden_size = ff_hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.rope = RoPE(dim=d_model, base=10000.0)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout, batch_first=True)  # makes input (batch, seq, dim) instead of (seq, batch, dim)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_hidden_size),\n",
    "            nn.GLU(dim=-1),  # https://arxiv.org/pdf/2002.05202; please, check the last line of section 4 (Conclusions)\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_hidden_size // 2, d_model)\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention sub-layer with residual connection\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        q, k = self.rope(x, seq_len=x.shape[1]), self.rope(x, seq_len=x.shape[1])\n",
    "        attn_output, _ = self.attention(query=q, key=k, value=x, need_weights=False)  # I do not intend to visualize attention\n",
    "        x = self.dropout_layer(attn_output) + residual\n",
    "\n",
    "        # FF sub-layer with residual connection\n",
    "        residual = x\n",
    "        x = self.ff(self.norm2(x))\n",
    "        x = self.dropout_layer(x) + residual\n",
    "\n",
    "        return x"
   ],
   "id": "ab6582aba969ce96",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`DeTention`, which is the complete end-to-end model that handles input projection from raw prices, stacks multiple `DeTention` blocks and produces the final prediction. It has raw stock prices to $\\mathbb{R}^{d_\\text{model}}$ projection, then uses $k$ `DeTentionBlock`s and optional average pooling for final prediction.",
   "id": "32d2a2a7652d39f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.384738Z",
     "start_time": "2025-12-21T14:44:04.378252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeTention(nn.Module):\n",
    "    def __init__(self, seq_len: int = 30, d_model: int = 64, n_heads: int = 4, n_layers: int = 2, ff_hidden_size: int = 256, dropout: float = 0.2, use_avg_pool: bool = True):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.use_avg_pool = use_avg_pool\n",
    "\n",
    "        self.input_proj = nn.Linear(1, d_model)  # 'Close' -> d_model\n",
    "        self.DeTention_blocks = nn.ModuleList([DeTentionBlock(d_model=d_model, n_heads=n_heads, ff_hidden_size=ff_hidden_size, dropout=dropout) for i in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_head = nn.Linear(d_model, 1)\n",
    "\n",
    "        # optional global average pooling instead of last token\n",
    "        if use_avg_pool:\n",
    "            self.pool = nn.AdaptiveAvgPool1d(output_size=1)  # (B, d_model, L) -> (B, d_model, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, 1) -> (batch_size, seq_len, d_model)\n",
    "        for block in self.DeTention_blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # pooling or last token\n",
    "        if self.use_avg_pool:\n",
    "            x = self.pool(x.transpose(1, 2)).squeeze(-1)  # (B, d_model, L) -> (B, d_model); nn.AdaptiveAvgPool1d expects channel dimension second (like images: N, C, L)\n",
    "        else:\n",
    "            x = x[:, -1, :]  # last position\n",
    "\n",
    "        x = self.output_head(x)  # next price prediction: (batch_size, 1)\n",
    "\n",
    "        return x.squeeze(dim=-1)  # (batch_size,)"
   ],
   "id": "f72fb5959462e68b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also have methods for saving and loading the `DeTention` model:",
   "id": "176e9c4ed9d0b1c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.397724Z",
     "start_time": "2025-12-21T14:44:04.392557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_model(model: DeTention, path: str = \"models/DeTention.pth\") -> None:  # save the entire model (architecture + weights)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'config': {\n",
    "            'seq_len': model.seq_len,\n",
    "            'd_model': model.d_model,\n",
    "            'use_avg_pool': model.use_avg_pool\n",
    "        }\n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_model(path: str = \"models/DeTention.pth\", **model_kwargs) -> DeTention:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {path}\")\n",
    "\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cpu'), weights_only=True)\n",
    "    config = checkpoint.get('config', {})\n",
    "    config.update(model_kwargs)\n",
    "    model = DeTention(**config)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model"
   ],
   "id": "d9678beb03601a78",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup for training\n",
    "\n",
    "First, let's declare hyperparameters:"
   ],
   "id": "5ada7afdce7b3068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:04.410584Z",
     "start_time": "2025-12-21T14:44:04.405751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ticker = \"PINS\"\n",
    "period = \"3y\"\n",
    "L = 30\n",
    "train_ratio = 0.8\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "epochs = 100\n",
    "patience = 15\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "ff_hidden_size = 256\n",
    "dropout = 0.2\n",
    "use_avg_pool = True\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_path = \"models/DeTention_best.pth\"\n",
    "final_path = \"models/DeTention.pth\"\n",
    "scaler_path = \"models/scaler.pkl\"\n",
    "\n",
    "# enforce divisibility constraints\n",
    "if d_model % n_heads != 0:\n",
    "    logger.info(\"d_model (%d) not divisible by n_heads (%d) -> resetting d_model to 64 and n_head to 4\", d_model, n_heads)\n",
    "    d_model = 64\n",
    "    n_heads = 4\n",
    "\n",
    "if ff_hidden_size & 1:\n",
    "    logger.info(\"ff_hidden_size (%d) must be even for GLU -> resetting to 256\", ff_hidden_size)\n",
    "    ff_hidden_size = 256"
   ],
   "id": "67d4c49cd80ce9da",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, I need to get data in shape and format to start training:",
   "id": "b89867ed4b09962a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:10.454840Z",
     "start_time": "2025-12-21T14:44:04.419357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = get_data(ticker, period=period)\n",
    "X_train, y_train, X_test, y_test, scaler = prepare_datasets(df, L=L, train_ratio=train_ratio)\n",
    "\n",
    "# set torch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(dim=-1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(dim=-1)\n",
    "\n",
    "logger.info(\"Tensors -> X_train: %s, y_train: %s | X_test: %s, y_test: %s\", X_train_t.shape, y_train_t.shape, X_test_t.shape, y_test_t.shape)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "logger.info(\"DataLoaders -> train batches: %d, test batches: %d\", len(train_loader), len(test_loader))\n",
    "\n",
    "# Model & training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(\"Using device: %s\", device)\n",
    "\n",
    "model = DeTention(seq_len=L, d_model=d_model, n_heads=n_heads, n_layers=n_layers, ff_hidden_size=ff_hidden_size, dropout=dropout, use_avg_pool=use_avg_pool).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "logger.info(\"Initialized DeTention -> layers: %d, d_model: %d, heads: %d\", n_layers, d_model, n_heads)"
   ],
   "id": "a816b18ea23d0f75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - HISTORY:\n",
      "['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "                                Open       High        Low      Close  \\\n",
      "Date                                                                    \n",
      "2022-12-20 00:00:00-05:00  23.910000  24.700001  23.719999  24.440001   \n",
      "2022-12-21 00:00:00-05:00  24.510000  25.385000  24.139999  25.139999   \n",
      "2022-12-22 00:00:00-05:00  24.559999  24.920000  24.080000  24.719999   \n",
      "2022-12-23 00:00:00-05:00  24.520000  24.920000  24.299999  24.530001   \n",
      "2022-12-27 00:00:00-05:00  24.200001  24.260000  23.410000  23.430000   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2022-12-20 00:00:00-05:00   7141600        0.0           0.0  \n",
      "2022-12-21 00:00:00-05:00  10472500        0.0           0.0  \n",
      "2022-12-22 00:00:00-05:00  11424100        0.0           0.0  \n",
      "2022-12-23 00:00:00-05:00   5994700        0.0           0.0  \n",
      "2022-12-27 00:00:00-05:00   7382000        0.0           0.0  \n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - Train scaled shape: (602, 1), Test scaled shape: (151, 1)\n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - Scaler min: -0.8587, scale: 0.0413\n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - Created windows -> X shape: (572, 30, 1), y shape: (572,)\n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - Created windows -> X shape: (121, 30, 1), y shape: (121,)\n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - Final datasets ready -> Train samples: 572, Test samples: 121\n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - Tensors -> X_train: torch.Size([572, 30, 1]), y_train: torch.Size([572, 1]) | X_test: torch.Size([121, 30, 1]), y_test: torch.Size([121, 1])\n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - DataLoaders -> train batches: 18, test batches: 4\n",
      "21/12/2025 18:44:08 - BackToTheTrainLogger (Train) - INFO - Using device: cpu\n",
      "21/12/2025 18:44:10 - BackToTheTrainLogger (Train) - INFO - Initialized DeTention -> layers: 2, d_model: 64, heads: 4\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Time for training!",
   "id": "70e9af0cb3f77272"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:58.968366Z",
     "start_time": "2025-12-21T14:44:10.462932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred, batch_y.squeeze(dim=-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y.squeeze(dim=-1))\n",
    "\n",
    "            test_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    logger.info(\"Epoch %03d | Train Loss: %.4f | Test Loss: %.4f\", epoch, train_loss, test_loss)\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 0\n",
    "        save_model(model, path=best_path)\n",
    "        logger.info(\"New best model saved (test loss: %.4f)\", best_test_loss)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(\"Early stopping triggered after epoch %d\", epoch)\n",
    "            break"
   ],
   "id": "fdd4e3f4cca64eb0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/2025 18:44:11 - BackToTheTrainLogger (Train) - INFO - Epoch 001 | Train Loss: 0.1574 | Test Loss: 0.0472\n",
      "21/12/2025 18:44:11 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0472)\n",
      "21/12/2025 18:44:12 - BackToTheTrainLogger (Train) - INFO - Epoch 002 | Train Loss: 0.0478 | Test Loss: 0.0259\n",
      "21/12/2025 18:44:12 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0259)\n",
      "21/12/2025 18:44:13 - BackToTheTrainLogger (Train) - INFO - Epoch 003 | Train Loss: 0.0193 | Test Loss: 0.0257\n",
      "21/12/2025 18:44:13 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0257)\n",
      "21/12/2025 18:44:14 - BackToTheTrainLogger (Train) - INFO - Epoch 004 | Train Loss: 0.0159 | Test Loss: 0.0218\n",
      "21/12/2025 18:44:14 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0218)\n",
      "21/12/2025 18:44:15 - BackToTheTrainLogger (Train) - INFO - Epoch 005 | Train Loss: 0.0146 | Test Loss: 0.0191\n",
      "21/12/2025 18:44:15 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0191)\n",
      "21/12/2025 18:44:16 - BackToTheTrainLogger (Train) - INFO - Epoch 006 | Train Loss: 0.0138 | Test Loss: 0.0247\n",
      "21/12/2025 18:44:17 - BackToTheTrainLogger (Train) - INFO - Epoch 007 | Train Loss: 0.0132 | Test Loss: 0.0126\n",
      "21/12/2025 18:44:17 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0126)\n",
      "21/12/2025 18:44:18 - BackToTheTrainLogger (Train) - INFO - Epoch 008 | Train Loss: 0.0129 | Test Loss: 0.0108\n",
      "21/12/2025 18:44:18 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0108)\n",
      "21/12/2025 18:44:19 - BackToTheTrainLogger (Train) - INFO - Epoch 009 | Train Loss: 0.0118 | Test Loss: 0.0267\n",
      "21/12/2025 18:44:20 - BackToTheTrainLogger (Train) - INFO - Epoch 010 | Train Loss: 0.0100 | Test Loss: 0.0097\n",
      "21/12/2025 18:44:20 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0097)\n",
      "21/12/2025 18:44:21 - BackToTheTrainLogger (Train) - INFO - Epoch 011 | Train Loss: 0.0098 | Test Loss: 0.0091\n",
      "21/12/2025 18:44:21 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0091)\n",
      "21/12/2025 18:44:22 - BackToTheTrainLogger (Train) - INFO - Epoch 012 | Train Loss: 0.0081 | Test Loss: 0.0146\n",
      "21/12/2025 18:44:22 - BackToTheTrainLogger (Train) - INFO - Epoch 013 | Train Loss: 0.0077 | Test Loss: 0.0086\n",
      "21/12/2025 18:44:22 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0086)\n",
      "21/12/2025 18:44:23 - BackToTheTrainLogger (Train) - INFO - Epoch 014 | Train Loss: 0.0075 | Test Loss: 0.0077\n",
      "21/12/2025 18:44:23 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0077)\n",
      "21/12/2025 18:44:24 - BackToTheTrainLogger (Train) - INFO - Epoch 015 | Train Loss: 0.0064 | Test Loss: 0.0125\n",
      "21/12/2025 18:44:24 - BackToTheTrainLogger (Train) - INFO - Epoch 016 | Train Loss: 0.0057 | Test Loss: 0.0124\n",
      "21/12/2025 18:44:25 - BackToTheTrainLogger (Train) - INFO - Epoch 017 | Train Loss: 0.0055 | Test Loss: 0.0090\n",
      "21/12/2025 18:44:26 - BackToTheTrainLogger (Train) - INFO - Epoch 018 | Train Loss: 0.0051 | Test Loss: 0.0101\n",
      "21/12/2025 18:44:27 - BackToTheTrainLogger (Train) - INFO - Epoch 019 | Train Loss: 0.0052 | Test Loss: 0.0087\n",
      "21/12/2025 18:44:27 - BackToTheTrainLogger (Train) - INFO - Epoch 020 | Train Loss: 0.0054 | Test Loss: 0.0154\n",
      "21/12/2025 18:44:28 - BackToTheTrainLogger (Train) - INFO - Epoch 021 | Train Loss: 0.0050 | Test Loss: 0.0080\n",
      "21/12/2025 18:44:29 - BackToTheTrainLogger (Train) - INFO - Epoch 022 | Train Loss: 0.0046 | Test Loss: 0.0055\n",
      "21/12/2025 18:44:29 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0055)\n",
      "21/12/2025 18:44:29 - BackToTheTrainLogger (Train) - INFO - Epoch 023 | Train Loss: 0.0044 | Test Loss: 0.0078\n",
      "21/12/2025 18:44:30 - BackToTheTrainLogger (Train) - INFO - Epoch 024 | Train Loss: 0.0041 | Test Loss: 0.0100\n",
      "21/12/2025 18:44:30 - BackToTheTrainLogger (Train) - INFO - Epoch 025 | Train Loss: 0.0039 | Test Loss: 0.0080\n",
      "21/12/2025 18:44:31 - BackToTheTrainLogger (Train) - INFO - Epoch 026 | Train Loss: 0.0038 | Test Loss: 0.0055\n",
      "21/12/2025 18:44:31 - BackToTheTrainLogger (Train) - INFO - Epoch 027 | Train Loss: 0.0040 | Test Loss: 0.0106\n",
      "21/12/2025 18:44:32 - BackToTheTrainLogger (Train) - INFO - Epoch 028 | Train Loss: 0.0036 | Test Loss: 0.0080\n",
      "21/12/2025 18:44:32 - BackToTheTrainLogger (Train) - INFO - Epoch 029 | Train Loss: 0.0034 | Test Loss: 0.0057\n",
      "21/12/2025 18:44:33 - BackToTheTrainLogger (Train) - INFO - Epoch 030 | Train Loss: 0.0036 | Test Loss: 0.0114\n",
      "21/12/2025 18:44:34 - BackToTheTrainLogger (Train) - INFO - Epoch 031 | Train Loss: 0.0036 | Test Loss: 0.0055\n",
      "21/12/2025 18:44:34 - BackToTheTrainLogger (Train) - INFO - Epoch 032 | Train Loss: 0.0034 | Test Loss: 0.0080\n",
      "21/12/2025 18:44:35 - BackToTheTrainLogger (Train) - INFO - Epoch 033 | Train Loss: 0.0034 | Test Loss: 0.0078\n",
      "21/12/2025 18:44:35 - BackToTheTrainLogger (Train) - INFO - Epoch 034 | Train Loss: 0.0033 | Test Loss: 0.0081\n",
      "21/12/2025 18:44:36 - BackToTheTrainLogger (Train) - INFO - Epoch 035 | Train Loss: 0.0033 | Test Loss: 0.0097\n",
      "21/12/2025 18:44:37 - BackToTheTrainLogger (Train) - INFO - Epoch 036 | Train Loss: 0.0035 | Test Loss: 0.0047\n",
      "21/12/2025 18:44:37 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0047)\n",
      "21/12/2025 18:44:37 - BackToTheTrainLogger (Train) - INFO - Epoch 037 | Train Loss: 0.0033 | Test Loss: 0.0093\n",
      "21/12/2025 18:44:38 - BackToTheTrainLogger (Train) - INFO - Epoch 038 | Train Loss: 0.0034 | Test Loss: 0.0053\n",
      "21/12/2025 18:44:38 - BackToTheTrainLogger (Train) - INFO - Epoch 039 | Train Loss: 0.0031 | Test Loss: 0.0088\n",
      "21/12/2025 18:44:39 - BackToTheTrainLogger (Train) - INFO - Epoch 040 | Train Loss: 0.0032 | Test Loss: 0.0070\n",
      "21/12/2025 18:44:39 - BackToTheTrainLogger (Train) - INFO - Epoch 041 | Train Loss: 0.0029 | Test Loss: 0.0041\n",
      "21/12/2025 18:44:39 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0041)\n",
      "21/12/2025 18:44:40 - BackToTheTrainLogger (Train) - INFO - Epoch 042 | Train Loss: 0.0033 | Test Loss: 0.0067\n",
      "21/12/2025 18:44:41 - BackToTheTrainLogger (Train) - INFO - Epoch 043 | Train Loss: 0.0028 | Test Loss: 0.0075\n",
      "21/12/2025 18:44:41 - BackToTheTrainLogger (Train) - INFO - Epoch 044 | Train Loss: 0.0031 | Test Loss: 0.0070\n",
      "21/12/2025 18:44:42 - BackToTheTrainLogger (Train) - INFO - Epoch 045 | Train Loss: 0.0029 | Test Loss: 0.0072\n",
      "21/12/2025 18:44:43 - BackToTheTrainLogger (Train) - INFO - Epoch 046 | Train Loss: 0.0026 | Test Loss: 0.0051\n",
      "21/12/2025 18:44:43 - BackToTheTrainLogger (Train) - INFO - Epoch 047 | Train Loss: 0.0026 | Test Loss: 0.0050\n",
      "21/12/2025 18:44:44 - BackToTheTrainLogger (Train) - INFO - Epoch 048 | Train Loss: 0.0026 | Test Loss: 0.0086\n",
      "21/12/2025 18:44:45 - BackToTheTrainLogger (Train) - INFO - Epoch 049 | Train Loss: 0.0027 | Test Loss: 0.0042\n",
      "21/12/2025 18:44:45 - BackToTheTrainLogger (Train) - INFO - Epoch 050 | Train Loss: 0.0027 | Test Loss: 0.0082\n",
      "21/12/2025 18:44:46 - BackToTheTrainLogger (Train) - INFO - Epoch 051 | Train Loss: 0.0027 | Test Loss: 0.0054\n",
      "21/12/2025 18:44:46 - BackToTheTrainLogger (Train) - INFO - Epoch 052 | Train Loss: 0.0027 | Test Loss: 0.0071\n",
      "21/12/2025 18:44:47 - BackToTheTrainLogger (Train) - INFO - Epoch 053 | Train Loss: 0.0026 | Test Loss: 0.0040\n",
      "21/12/2025 18:44:47 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0040)\n",
      "21/12/2025 18:44:48 - BackToTheTrainLogger (Train) - INFO - Epoch 054 | Train Loss: 0.0025 | Test Loss: 0.0063\n",
      "21/12/2025 18:44:48 - BackToTheTrainLogger (Train) - INFO - Epoch 055 | Train Loss: 0.0026 | Test Loss: 0.0050\n",
      "21/12/2025 18:44:49 - BackToTheTrainLogger (Train) - INFO - Epoch 056 | Train Loss: 0.0024 | Test Loss: 0.0067\n",
      "21/12/2025 18:44:49 - BackToTheTrainLogger (Train) - INFO - Epoch 057 | Train Loss: 0.0026 | Test Loss: 0.0073\n",
      "21/12/2025 18:44:50 - BackToTheTrainLogger (Train) - INFO - Epoch 058 | Train Loss: 0.0027 | Test Loss: 0.0032\n",
      "21/12/2025 18:44:50 - BackToTheTrainLogger (Train) - INFO - New best model saved (test loss: 0.0032)\n",
      "21/12/2025 18:44:50 - BackToTheTrainLogger (Train) - INFO - Epoch 059 | Train Loss: 0.0024 | Test Loss: 0.0062\n",
      "21/12/2025 18:44:51 - BackToTheTrainLogger (Train) - INFO - Epoch 060 | Train Loss: 0.0026 | Test Loss: 0.0039\n",
      "21/12/2025 18:44:51 - BackToTheTrainLogger (Train) - INFO - Epoch 061 | Train Loss: 0.0025 | Test Loss: 0.0062\n",
      "21/12/2025 18:44:52 - BackToTheTrainLogger (Train) - INFO - Epoch 062 | Train Loss: 0.0024 | Test Loss: 0.0075\n",
      "21/12/2025 18:44:52 - BackToTheTrainLogger (Train) - INFO - Epoch 063 | Train Loss: 0.0026 | Test Loss: 0.0050\n",
      "21/12/2025 18:44:53 - BackToTheTrainLogger (Train) - INFO - Epoch 064 | Train Loss: 0.0024 | Test Loss: 0.0055\n",
      "21/12/2025 18:44:54 - BackToTheTrainLogger (Train) - INFO - Epoch 065 | Train Loss: 0.0024 | Test Loss: 0.0048\n",
      "21/12/2025 18:44:54 - BackToTheTrainLogger (Train) - INFO - Epoch 066 | Train Loss: 0.0022 | Test Loss: 0.0056\n",
      "21/12/2025 18:44:55 - BackToTheTrainLogger (Train) - INFO - Epoch 067 | Train Loss: 0.0026 | Test Loss: 0.0035\n",
      "21/12/2025 18:44:56 - BackToTheTrainLogger (Train) - INFO - Epoch 068 | Train Loss: 0.0023 | Test Loss: 0.0056\n",
      "21/12/2025 18:44:56 - BackToTheTrainLogger (Train) - INFO - Epoch 069 | Train Loss: 0.0022 | Test Loss: 0.0038\n",
      "21/12/2025 18:44:57 - BackToTheTrainLogger (Train) - INFO - Epoch 070 | Train Loss: 0.0023 | Test Loss: 0.0046\n",
      "21/12/2025 18:44:57 - BackToTheTrainLogger (Train) - INFO - Epoch 071 | Train Loss: 0.0025 | Test Loss: 0.0067\n",
      "21/12/2025 18:44:58 - BackToTheTrainLogger (Train) - INFO - Epoch 072 | Train Loss: 0.0024 | Test Loss: 0.0067\n",
      "21/12/2025 18:44:58 - BackToTheTrainLogger (Train) - INFO - Epoch 073 | Train Loss: 0.0021 | Test Loss: 0.0037\n",
      "21/12/2025 18:44:58 - BackToTheTrainLogger (Train) - INFO - Early stopping triggered after epoch 73\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Metrics look promising, but can the hold up to expectations in `inference.ipynb`?\n",
    "\n",
    "Saving the best model:"
   ],
   "id": "f5eacf78a5e05b0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T14:44:59.008624Z",
     "start_time": "2025-12-21T14:44:58.978919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load best (temporary) model and save as final, then remove temporary best\n",
    "best_model = load_model(path=best_path)\n",
    "save_model(best_model, path=final_path)\n",
    "os.remove(best_path)\n",
    "\n",
    "# Save the scaler for inference\n",
    "joblib.dump(scaler, scaler_path)\n",
    "logger.info(\"Training completed. Final model saved as '%s' (temporary best model removed)\", final_path)\n",
    "logger.info(\"Scaler saved as '%s'\", scaler_path)"
   ],
   "id": "7d24c2ffb8b2024e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/2025 18:44:59 - BackToTheTrainLogger (Train) - INFO - Training completed. Final model saved as 'models/DeTention.pth' (temporary best model removed)\n",
      "21/12/2025 18:44:59 - BackToTheTrainLogger (Train) - INFO - Scaler saved as 'models/scaler.pkl'\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
